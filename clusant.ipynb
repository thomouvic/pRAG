{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from scipy.spatial.distance import cityblock\n",
    "from scipy.spatial.distance import pdist\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_file_path='glove.6B.300d.txt'):\n",
    "    embeddings = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        # Find the index of the first numerical token\n",
    "        embedding_start_index = None\n",
    "        for i, token in enumerate(first_line.split()):\n",
    "            try:\n",
    "                float(token)  # Attempt to convert the token to a float\n",
    "                embedding_start_index = i\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if embedding_start_index is None:\n",
    "            raise ValueError(\"Could not determine the start of the embedding vector in the first line.\")\n",
    "        \n",
    "        embedding_dim = len(first_line.split()) - embedding_start_index  # Determine the embedding dimension\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        \n",
    "        # Reset the file pointer to the beginning of the file\n",
    "        f.seek(0)\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = ' '.join(values[:-embedding_dim])  # Reconstruct the word\n",
    "            vector = list(map(float, values[-embedding_dim:]))  # Extract the embedding vector\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = load_glove_embeddings(glove_file_path='embeddings_countries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('åland islands', -0.0), ('faroe islands', -49.706144089999995), ('cayman islands', -56.54005485), ('solomon islands', -59.75879071), ('marshall islands', -61.417534605), ('northern mariana islands', -63.68305945166667), ('cocos (keeling) islands', -66.449814969), ('cook islands', -66.69857771), ('turks and caicos islands', -67.32165119500002), ('falkland islands (malvinas)', -67.854533625), ('virgin islands (british)', -68.38514268166668), ('heard island and mcdonald islands', -70.663665195), ('virgin islands (u.s.)', -71.39904455266667), ('united states minor outlying islands', -73.85662849299999), ('south georgia and the south sandwich islands', -81.29605458071428), ('bouvet island', -81.565424485), ('norfolk island', -82.820635275), ('antigua and barbuda', -83.66415023833332), ('christmas island', -83.969834265), ('british indian ocean territory', -84.5136254225)]\n",
      "[('bangladesh', -0.0), ('pakistan', -86.06863594000001), ('sri lanka', -86.60521655), ('india', -89.79430434), ('nepal', -91.34529721), ('maldives', -93.55023359), ('zimbabwe', -97.31725756), ('british indian ocean territory', -98.3237130775), ('united kingdom of great britain and northern ireland', -98.77946297874999), ('trinidad and tobago', -99.77603080666667), ('south georgia and the south sandwich islands', -99.82501313714286), ('papua new guinea', -100.22406082666667), ('taiwan, province of china', -100.56583472333332), ('namibia', -101.23105521), ('tanzania, united republic of', -101.29523357333333), ('moldova (republic of)', -101.40594642), ('congo (republic of the)', -101.4418627325), ('malaysia', -101.47332739000001), ('central african republic', -101.52576651666666), ('bolivia (plurinational state of)', -101.88289289875001)]\n"
     ]
    }
   ],
   "source": [
    "# This code is not strictly necessary, but it only serves to see if the embeddings make sense\n",
    "\n",
    "def get_top_similarities(target_word, embeddings, top_n=20):\n",
    "    target_embedding = embeddings.get(target_word)\n",
    "    if target_embedding is None:\n",
    "        return None\n",
    "\n",
    "    similarities = {}\n",
    "    for word, word_embedding in embeddings.items():\n",
    "        similarities[word] = -cityblock(target_embedding, word_embedding)\n",
    "\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]\n",
    "\n",
    "\n",
    "\n",
    "# example usage\n",
    "top_20_similarities = get_top_similarities('åland islands', embeddings)\n",
    "print(top_20_similarities)\n",
    "\n",
    "top_20_similarities = get_top_similarities('bangladesh', embeddings)\n",
    "print(top_20_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: argentina, brazil, chile, colombia, costa rica, cuba, dominican republic, ecuador, el salvador, guatemala, haiti, honduras, mexico, nicaragua, paraguay, peru, uruguay\n",
      "Cluster 1: australia, bangladesh, bhutan, cambodia, china, hong kong, india, indonesia, japan, malaysia, myanmar, nepal, pakistan, philippines, singapore, sri lanka, thailand, vietnam\n",
      "Cluster 2: bolivia (plurinational state of), bosnia and herzegovina, canada, central african republic, congo (republic of the), congo (democratic republic of the), french southern territories, georgia, vatican city state, korea (democratic people's republic of), korea (republic of), lao people's democratic republic, macedonia (the former yugoslav republic of), moldova (republic of), palestine, state of, russian federation, south africa, south sudan, taiwan, province of china, tanzania, united republic of, united kingdom of great britain and northern ireland, united states of america, venezuela (bolivarian republic of), western sahara\n",
      "Cluster 3: afghanistan, azerbaijan, bahrain, egypt, iran, iraq, israel, jordan, kazakhstan, kuwait, kyrgyzstan, lebanon, libya, mongolia, morocco, oman, qatar, saudi arabia, syrian arab republic, tajikistan, turkey, turkmenistan, united arab emirates, uzbekistan, yemen\n",
      "Cluster 4: anguilla, antigua and barbuda, aruba, bahamas, barbados, belize, bermuda, cayman islands, curaçao, dominica, grenada, guadeloupe, guyana, jamaica, martinique, montserrat, panama, suriname\n",
      "Cluster 5: american samoa, antarctica, bouvet island, british indian ocean territory, christmas island, cook islands, greenland, heard island and mcdonald islands, isle of man, jersey, marshall islands, micronesia (federated states of), new caledonia, new zealand, norfolk island, northern mariana islands, papua new guinea, solomon islands, south georgia and the south sandwich islands, turks and caicos islands, united states minor outlying islands, virgin islands (british), virgin islands (u.s.)\n",
      "Cluster 6: bonaire, sint eustatius and saba, french guiana, french polynesia, gibraltar, guernsey, macao, monaco, puerto rico, saint barthélemy, saint helena, ascension and tristan da cunha, saint kitts and nevis, saint lucia, saint martin (french part), saint pierre and miquelon, saint vincent and the grenadines, san marino, sao tome and principe, sint maarten (dutch part), svalbard and jan mayen, trinidad and tobago, wallis and futuna\n",
      "Cluster 7: algeria, benin, burkina faso, cameroon, chad, côte d'ivoire, djibouti, equatorial guinea, gabon, ghana, guinea, liberia, mali, mauritania, niger, nigeria, senegal, sierra leone, togo, tunisia\n",
      "Cluster 8: angola, botswana, burundi, eritrea, ethiopia, kenya, lesotho, madagascar, malawi, mozambique, namibia, rwanda, somalia, sudan, swaziland, uganda, zambia, zimbabwe\n",
      "Cluster 9: austria, belgium, denmark, finland, france, germany, iceland, ireland, italy, luxembourg, netherlands, norway, portugal, spain, sweden, switzerland\n",
      "Cluster 10: albania, armenia, belarus, bulgaria, croatia, cyprus, czech republic, estonia, greece, hungary, latvia, lithuania, montenegro, poland, romania, serbia, slovakia, slovenia, ukraine\n",
      "Cluster 11: åland islands, andorra, brunei darussalam, cabo verde, cocos (keeling) islands, comoros, falkland islands (malvinas), faroe islands, gambia, guam, guinea-bissau, liechtenstein, maldives, malta, mauritius, mayotte, réunion, seychelles, timor-leste\n",
      "Cluster 12: fiji, kiribati, nauru, niue, palau, pitcairn, samoa, tokelau, tonga, tuvalu, vanuatu\n"
     ]
    }
   ],
   "source": [
    "def bisecting_kmeans(embeddings, num_clusters):\n",
    "    # Initialize with all data in one cluster\n",
    "    clusters = {0: list(embeddings.keys())}\n",
    "    current_num_clusters = 1\n",
    "\n",
    "    while current_num_clusters < num_clusters:\n",
    "        # Find the largest cluster to split\n",
    "        largest_cluster_label = max(clusters, key=lambda label: len(clusters[label]))\n",
    "        largest_cluster = clusters.pop(largest_cluster_label)\n",
    "\n",
    "        # Extract embeddings for the largest cluster\n",
    "        largest_cluster_embeddings = [embeddings[word] for word in largest_cluster]\n",
    "\n",
    "        # Perform K-means with K=2 on the largest cluster\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        kmeans.fit(largest_cluster_embeddings)\n",
    "\n",
    "        # Split the cluster into two new clusters\n",
    "        new_cluster_1 = [largest_cluster[i] for i, label in enumerate(kmeans.labels_) if label == 0]\n",
    "        new_cluster_2 = [largest_cluster[i] for i, label in enumerate(kmeans.labels_) if label == 1]\n",
    "\n",
    "        # Add the new clusters to the clusters dictionary\n",
    "        clusters[current_num_clusters] = new_cluster_1\n",
    "        clusters[current_num_clusters + 1] = new_cluster_2\n",
    "\n",
    "        # Update the number of clusters\n",
    "        current_num_clusters += 2\n",
    "\n",
    "    # Reassign cluster labels to be continuous from 0 to num_clusters - 1\n",
    "    new_clusters = {i: clusters[label] for i, label in enumerate(sorted(clusters.keys()))}\n",
    "\n",
    "    return new_clusters\n",
    "\n",
    "# Example usage\n",
    "\n",
    "number_of_words = len(embeddings.keys())\n",
    "avg_words_per_cluster = 10\n",
    "num_clusters = number_of_words / avg_words_per_cluster\n",
    "clusters = bisecting_kmeans(embeddings, num_clusters)\n",
    "\n",
    "# Print clusters\n",
    "for label, words in clusters.items():\n",
    "    print(f\"Cluster {label}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'france' belongs to cluster 9.\n"
     ]
    }
   ],
   "source": [
    "# Given a word, find the cluster to which it belongs\n",
    "def find_word_cluster(word, clusters):\n",
    "    for label, words in clusters.items():\n",
    "        if word in words:\n",
    "            return label\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "target_word = 'france'\n",
    "target_cluster = find_word_cluster(target_word, clusters)\n",
    "if target_cluster is not None:\n",
    "    print(f\"Word '{target_word}' belongs to cluster {target_cluster}.\")\n",
    "else:\n",
    "    print(f\"Word '{target_word}' does not belong to any cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max intercluster distance: 88.66460151975168\n",
      "Selected Cluster 9: austria, belgium, denmark, finland, france, germany, iceland, ireland, italy, luxembourg, netherlands, norway, portugal, spain, sweden, switzerland\n"
     ]
    }
   ],
   "source": [
    "def compute_largest_intercluster_distance(clusters, embeddings):\n",
    "    K = 1 # amplification factor, we didn't need it bigger than 1\n",
    "\n",
    "    # Calculate centroids of each cluster\n",
    "    centroids = {}\n",
    "    for label, words in clusters.items():\n",
    "        cluster_vectors = [embeddings[word] for word in words]\n",
    "        centroids[label] = K * np.mean(cluster_vectors, axis=0)\n",
    "\n",
    "    # Compute pairwise distances between centroids\n",
    "    centroid_labels = list(centroids.keys())\n",
    "    centroid_vectors = [centroids[label] for label in centroid_labels]\n",
    "    distances = manhattan_distances(centroid_vectors, centroid_vectors)\n",
    "\n",
    "    # Find the maximum distance\n",
    "    sensitivity_inter = np.max(distances)\n",
    "    \n",
    "    return distances, sensitivity_inter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Metric DP, sensitivity = 1\n",
    "def exponential_mechanism_for_cluster(clusters, distances, selected_label, epsilon=1):\n",
    "    # Calculate the utilities for each cluster based on the negative distance to the selected cluster\n",
    "    utilities = np.array([-distances[selected_label, label] for label in clusters.keys()])\n",
    "\n",
    "    # sensitivity = np.max(distances)\n",
    "    sensitivity = 1\n",
    "\n",
    "    # Compute the probabilities for each cluster using the exponential mechanism\n",
    "    probabilities = np.exp(utilities * epsilon / (2 * sensitivity))\n",
    "    probabilities /= np.sum(probabilities)\n",
    "\n",
    "    # Select a cluster probabilistically\n",
    "    selected_cluster = np.random.choice(list(clusters.keys()), p=probabilities)\n",
    "\n",
    "    return selected_cluster\n",
    "\n",
    "\n",
    "distances_inter, sensitivity_inter = compute_largest_intercluster_distance(clusters, embeddings)\n",
    "print(f\"Max intercluster distance: {sensitivity_inter}\")\n",
    "\n",
    "selected_label = target_cluster  # The label of the cluster for which we want to select a similar cluster\n",
    "selected_cluster = exponential_mechanism_for_cluster(clusters, distances_inter, selected_label, epsilon=1)\n",
    "print(f\"Selected Cluster {selected_cluster}: {', '.join(clusters[selected_cluster])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected word: netherlands\n"
     ]
    }
   ],
   "source": [
    "def compute_largest_intracluster_distance(clusters, embeddings):\n",
    "    sensitivity_intra = {}\n",
    "    for label, words in clusters.items():\n",
    "        # Get the embeddings for the words in the cluster\n",
    "        cluster_embeddings = [embeddings[word] for word in words]\n",
    "        \n",
    "        # Compute the pairwise distances within the cluster\n",
    "        if len(cluster_embeddings) > 1:\n",
    "            distances = pdist(cluster_embeddings, metric='cityblock')\n",
    "            # Find the maximum distance\n",
    "            max_distance = max(distances)\n",
    "        else:\n",
    "            # If the cluster has only one element, set the max distance to 0\n",
    "            max_distance = 0\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        sensitivity_intra[label] = max_distance\n",
    "    \n",
    "    return sensitivity_intra\n",
    "\n",
    "\n",
    "\n",
    "def exponential_mechanism_for_word(selected_cluster_embeddings, target_word_embedding, sensitivity_intra, epsilon=1):\n",
    "    # calculate the utility for each word based on the negative\n",
    "    utilities = [-cityblock(target_word_embedding, word_embedding) for word_embedding in selected_cluster_embeddings]\n",
    "\n",
    "    # Compute the probabilities for each word using the exponential mechanism\n",
    "    probabilities = np.exp(epsilon * np.array(utilities) / (2 * sensitivity_intra))\n",
    "    probabilities /= np.sum(probabilities)\n",
    "\n",
    "    # Randomly select a word based on the probabilities\n",
    "    selected_index = np.random.choice(range(len(selected_cluster_embeddings)), p=probabilities)\n",
    "    return selected_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_word_embedding = embeddings[target_word]\n",
    "selected_cluster_embeddings = [embeddings[word] for word in clusters[selected_cluster]]\n",
    "sensitivity_intra = compute_largest_intracluster_distance(clusters, embeddings) # dict\n",
    "\n",
    "selected_index = exponential_mechanism_for_word(selected_cluster_embeddings, target_word_embedding, sensitivity_intra[selected_cluster], epsilon=1)\n",
    "selected_word = clusters[selected_cluster][selected_index]\n",
    "\n",
    "print(f\"Selected word: {selected_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 300\n",
      "replacement_dict:  {'afghanistan': 'iran', 'åland islands': 'falkland islands (malvinas)', 'albania': 'czech republic', 'algeria': 'cameroon', 'american samoa': 'south georgia and the south sandwich islands', 'andorra': 'mayotte', 'angola': 'zimbabwe', 'anguilla': 'guyana', 'antarctica': 'south georgia and the south sandwich islands', 'antigua and barbuda': 'bahamas', 'argentina': 'argentina', 'armenia': 'belarus', 'aruba': 'dominica', 'australia': 'bangladesh', 'austria': 'portugal', 'azerbaijan': 'azerbaijan', 'bahamas': 'montserrat', 'bahrain': 'iraq', 'bangladesh': 'india', 'barbados': 'montserrat', 'belarus': 'lithuania', 'belgium': 'portugal', 'belize': 'montserrat', 'benin': 'ghana', 'bermuda': 'panama', 'bhutan': 'china', 'bolivia (plurinational state of)': 'palestine, state of', 'bonaire, sint eustatius and saba': 'saint lucia', 'bosnia and herzegovina': 'venezuela (bolivarian republic of)', 'botswana': 'uganda', 'bouvet island': 'virgin islands (british)', 'brazil': 'uruguay', 'british indian ocean territory': 'virgin islands (british)', 'brunei darussalam': 'timor-leste', 'bulgaria': 'croatia', 'burkina faso': 'equatorial guinea', 'burundi': 'kenya', 'cabo verde': 'cocos (keeling) islands', 'cambodia': 'bhutan', 'cameroon': 'togo', 'canada': 'macedonia (the former yugoslav republic of)', 'cayman islands': 'bermuda', 'central african republic': 'moldova (republic of)', 'chad': 'cameroon', 'chile': 'honduras', 'china': 'philippines', 'christmas island': 'british indian ocean territory', 'cocos (keeling) islands': 'réunion', 'colombia': 'chile', 'comoros': 'cabo verde', 'congo (republic of the)': 'congo (republic of the)', 'congo (democratic republic of the)': 'georgia', 'cook islands': 'marshall islands', 'costa rica': 'haiti', \"côte d'ivoire\": 'togo', 'croatia': 'lithuania', 'cuba': 'argentina', 'curaçao': 'barbados', 'cyprus': 'serbia', 'czech republic': 'albania', 'denmark': 'netherlands', 'djibouti': 'niger', 'dominica': 'bermuda', 'dominican republic': 'brazil', 'ecuador': 'argentina', 'egypt': 'united arab emirates', 'el salvador': 'costa rica', 'equatorial guinea': 'chad', 'eritrea': 'eritrea', 'estonia': 'belarus', 'ethiopia': 'angola', 'falkland islands (malvinas)': 'brunei darussalam', 'faroe islands': 'faroe islands', 'fiji': 'nauru', 'finland': 'finland', 'france': 'luxembourg', 'french guiana': 'sao tome and principe', 'french polynesia': 'gibraltar', 'french southern territories': 'bolivia (plurinational state of)', 'gabon': 'djibouti', 'gambia': 'liechtenstein', 'georgia': 'western sahara', 'germany': 'germany', 'ghana': 'benin', 'gibraltar': 'saint barthélemy', 'greece': 'greece', 'greenland': 'virgin islands (u.s.)', 'grenada': 'aruba', 'guadeloupe': 'aruba', 'guam': 'timor-leste', 'guatemala': 'guatemala', 'guernsey': 'gibraltar', 'guinea': 'benin', 'guinea-bissau': 'faroe islands', 'guyana': 'anguilla', 'haiti': 'haiti', 'heard island and mcdonald islands': 'papua new guinea', 'vatican city state': 'palestine, state of', 'honduras': 'ecuador', 'hong kong': 'nepal', 'hungary': 'greece', 'iceland': 'austria', 'india': 'nepal', 'indonesia': 'nepal', 'iran': 'bahrain', 'iraq': 'lebanon', 'ireland': 'denmark', 'isle of man': 'turks and caicos islands', 'israel': 'turkey', 'italy': 'denmark', 'jamaica': 'montserrat', 'japan': 'japan', 'jersey': 'papua new guinea', 'jordan': 'iran', 'kazakhstan': 'egypt', 'kenya': 'burundi', 'kiribati': 'palau', \"korea (democratic people's republic of)\": \"korea (democratic people's republic of)\", 'korea (republic of)': 'moldova (republic of)', 'kuwait': 'lebanon', 'kyrgyzstan': 'kazakhstan', \"lao people's democratic republic\": 'korea (republic of)', 'latvia': 'latvia', 'lebanon': 'azerbaijan', 'lesotho': 'mozambique', 'liberia': 'liberia', 'libya': 'iran', 'liechtenstein': 'gambia', 'lithuania': 'cyprus', 'luxembourg': 'iceland', 'macao': 'monaco', 'macedonia (the former yugoslav republic of)': 'congo (republic of the)', 'madagascar': 'angola', 'malawi': 'zambia', 'malaysia': 'china', 'maldives': 'falkland islands (malvinas)', 'mali': 'senegal', 'malta': 'gambia', 'marshall islands': 'solomon islands', 'martinique': 'grenada', 'mauritania': 'algeria', 'mauritius': 'guam', 'mayotte': 'liechtenstein', 'mexico': 'dominican republic', 'micronesia (federated states of)': 'greenland', 'moldova (republic of)': 'palestine, state of', 'monaco': 'saint pierre and miquelon', 'mongolia': 'oman', 'montenegro': 'poland', 'montserrat': 'guadeloupe', 'morocco': 'mongolia', 'mozambique': 'angola', 'myanmar': 'cambodia', 'namibia': 'mozambique', 'nauru': 'pitcairn', 'nepal': 'sri lanka', 'netherlands': 'italy', 'new caledonia': 'greenland', 'new zealand': 'greenland', 'nicaragua': 'honduras', 'niger': 'burkina faso', 'nigeria': 'liberia', 'niue': 'samoa', 'norfolk island': 'norfolk island', 'northern mariana islands': 'new caledonia', 'norway': 'spain', 'oman': 'mongolia', 'pakistan': 'india', 'palau': 'fiji', 'palestine, state of': 'congo (republic of the)', 'panama': 'martinique', 'papua new guinea': 'american samoa', 'paraguay': 'uruguay', 'peru': 'peru', 'philippines': 'myanmar', 'pitcairn': 'nauru', 'poland': 'hungary', 'portugal': 'portugal', 'puerto rico': 'saint kitts and nevis', 'qatar': 'turkmenistan', 'réunion': 'guinea-bissau', 'romania': 'slovenia', 'russian federation': 'palestine, state of', 'rwanda': 'madagascar', 'saint barthélemy': 'saint kitts and nevis', 'saint helena, ascension and tristan da cunha': 'monaco', 'saint kitts and nevis': 'saint lucia', 'saint lucia': 'puerto rico', 'saint martin (french part)': 'saint helena, ascension and tristan da cunha', 'saint pierre and miquelon': 'saint martin (french part)', 'saint vincent and the grenadines': 'wallis and futuna', 'samoa': 'palau', 'san marino': 'saint pierre and miquelon', 'sao tome and principe': 'monaco', 'saudi arabia': 'egypt', 'senegal': 'ghana', 'serbia': 'croatia', 'seychelles': 'falkland islands (malvinas)', 'sierra leone': 'equatorial guinea', 'singapore': 'bangladesh', 'sint maarten (dutch part)': 'saint barthélemy', 'slovakia': 'slovakia', 'slovenia': 'estonia', 'solomon islands': 'turks and caicos islands', 'somalia': 'swaziland', 'south africa': 'western sahara', 'south georgia and the south sandwich islands': 'turks and caicos islands', 'south sudan': 'moldova (republic of)', 'spain': 'ireland', 'sri lanka': 'philippines', 'sudan': 'zimbabwe', 'suriname': 'anguilla', 'svalbard and jan mayen': 'bonaire, sint eustatius and saba', 'swaziland': 'zambia', 'sweden': 'finland', 'switzerland': 'austria', 'syrian arab republic': 'qatar', 'taiwan, province of china': 'russian federation', 'tajikistan': 'saudi arabia', 'tanzania, united republic of': 'vatican city state', 'thailand': 'nepal', 'timor-leste': 'malta', 'togo': 'algeria', 'tokelau': 'tuvalu', 'tonga': 'samoa', 'trinidad and tobago': 'bonaire, sint eustatius and saba', 'tunisia': \"côte d'ivoire\", 'turkey': 'azerbaijan', 'turkmenistan': 'afghanistan', 'turks and caicos islands': 'papua new guinea', 'tuvalu': 'kiribati', 'uganda': 'sudan', 'ukraine': 'armenia', 'united arab emirates': 'libya', 'united kingdom of great britain and northern ireland': 'south sudan', 'united states minor outlying islands': 'british indian ocean territory', 'united states of america': 'south sudan', 'uruguay': 'mexico', 'uzbekistan': 'jordan', 'vanuatu': 'tonga', 'venezuela (bolivarian republic of)': 'central african republic', 'vietnam': 'nepal', 'virgin islands (british)': 'isle of man', 'virgin islands (u.s.)': 'american samoa', 'wallis and futuna': 'french polynesia', 'western sahara': 'korea (republic of)', 'yemen': 'united arab emirates', 'zambia': 'madagascar', 'zimbabwe': 'ethiopia'}\n"
     ]
    }
   ],
   "source": [
    "# run everything here\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = load_glove_embeddings(glove_file_path='embeddings_countries.txt')\n",
    "\n",
    "# create clusters\n",
    "number_of_words = len(embeddings.keys())\n",
    "avg_words_per_cluster = 10\n",
    "num_clusters = number_of_words / avg_words_per_cluster\n",
    "clusters = bisecting_kmeans(embeddings, num_clusters)\n",
    "\n",
    "\n",
    "def replace_word(target_word, clusters, embeddings):\n",
    "\n",
    "    target_cluster = find_word_cluster(target_word, clusters)\n",
    "    # print(f\"Target Cluster {target_cluster}: {', '.join(clusters[target_cluster])}\")\n",
    "\n",
    "    # select cluster\n",
    "    distances_inter, sensitivity_inter = compute_largest_intercluster_distance(clusters, embeddings)\n",
    "    selected_label = target_cluster  # The label of the cluster for which we want to select a similar cluster\n",
    "    selected_cluster = exponential_mechanism_for_cluster(clusters, distances_inter, selected_label, epsilon=1)\n",
    "    # print(f\"Select Cluster {selected_cluster}: {', '.join(clusters[selected_cluster])}\")\n",
    "\n",
    "    # select word\n",
    "    target_word_embedding = embeddings[target_word]\n",
    "    selected_cluster_embeddings = [embeddings[word] for word in clusters[selected_cluster]]\n",
    "    sensitivity_intra = compute_largest_intracluster_distance(clusters, embeddings) # dict\n",
    "\n",
    "    selected_index = exponential_mechanism_for_word(selected_cluster_embeddings, target_word_embedding, sensitivity_intra[selected_cluster], epsilon=1)\n",
    "    selected_word = clusters[selected_cluster][selected_index]\n",
    "\n",
    "    return selected_word\n",
    "\n",
    "def get_replacement_dict(embeddings, clusters):\n",
    "    replacement_dict = {}\n",
    "    for word in embeddings.keys():\n",
    "        replacement_dict[word] = replace_word(word, clusters, embeddings)\n",
    "    return replacement_dict\n",
    "\n",
    "replacement_dict = get_replacement_dict(embeddings, clusters)\n",
    "\n",
    "print('replacement_dict: ', replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am from sri lanka. I live in the falkland islands (malvinas).\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text, replacement_dict):\n",
    "    # Sort the keys by length in descending order\n",
    "    # This is necessary to ensure that longer keys are replaced first\n",
    "    # E.g., if we have 'mexico city' and 'mexico', we want to replace 'mexico city' first\n",
    "    sorted_keys = sorted(replacement_dict.keys(), key=len, reverse=True)\n",
    "    translated_text = text\n",
    "    for key in sorted_keys:\n",
    "        # Replace occurrences of the key in the text with its value\n",
    "        translated_text = translated_text.replace(key, replacement_dict[key])\n",
    "    return translated_text\n",
    "\n",
    "text = \"I am from bangladesh. I live in the åland islands.\"\n",
    "translated_text = translate_text(text, replacement_dict)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
